"""
–ú–û–î–£–õ–¨ –ê: –ê–Ω–∞–ª–∏–∑ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
–î–ª—è —á–µ–º–ø–∏–æ–Ω–∞—Ç–∞ "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—ã" –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ "–ù–µ–π—Ä–æ—Å–µ—Ç–∏ –∏ –±–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ"
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from PIL import Image
import cv2
from scipy import stats
from sklearn.preprocessing import StandardScaler
import warnings

warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 12
sns.set_style("whitegrid")

print("=" * 70)
print("–ú–û–î–£–õ–¨ –ê: –ê–ù–ê–õ–ò–ó –ò –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•")
print("=" * 70)


class ImageDataAnalyzer:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""

    def __init__(self, data_path="data"):
        self.data_path = data_path
        self.df = None
        self.report_data = {}

    def extract_image_features(self, image_path):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            with Image.open(image_path) as img:
                # –û—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
                width, height = img.size
                mode = img.mode
                format_type = img.format

                # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                img_cv = cv2.imread(str(image_path))

                if img_cv is None:
                    return None

                # –¶–≤–µ—Ç–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
                if len(img_cv.shape) == 3:
                    b, g, r = cv2.split(img_cv)
                    color_mean = [r.mean(), g.mean(), b.mean()]
                    color_std = [r.std(), g.std(), b.std()]
                else:
                    color_mean = [img_cv.mean()]
                    color_std = [img_cv.std()]

                # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —è—Ä–∫–æ—Å—Ç–∏ (–¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è)
                hist = cv2.calcHist([img_cv], [0], None, [256], [0, 256])
                hist = hist.flatten()

                return {
                    'filename': image_path.name,
                    'path': str(image_path.parent.name),
                    'width': width,
                    'height': height,
                    'aspect_ratio': width / height if height > 0 else 0,
                    'pixel_count': width * height,
                    'format': format_type if format_type else 'UNKNOWN',
                    'color_mode': mode,
                    'mean_intensity': np.mean(img_cv),
                    'std_intensity': np.std(img_cv),
                    'min_intensity': np.min(img_cv),
                    'max_intensity': np.max(img_cv),
                    'color_mean_r': color_mean[0] if len(color_mean) > 0 else 0,
                    'color_mean_g': color_mean[1] if len(color_mean) > 1 else 0,
                    'color_mean_b': color_mean[2] if len(color_mean) > 2 else 0,
                    'entropy': stats.entropy(hist) if hist.sum() > 0 else 0,
                    'is_face': 1 if 'faces' in str(image_path.parent) else 0
                }
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {image_path}: {e}")
            return None

    def load_and_analyze_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        print("\nüìä –ó–ê–ì–†–£–ó–ö–ê –ò –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•")
        print("-" * 50)

        # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        image_paths = []
        for root, dirs, files in os.walk(self.data_path):
            for file in files:
                if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):
                    image_paths.append(Path(root) / file)

        print(f"–ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(image_paths)}")

        if len(image_paths) == 0:
            print("‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
            return False

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        features_list = []
        for i, img_path in enumerate(image_paths):
            if i % 50 == 0:
                print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i}/{len(image_paths)}")
            features = self.extract_image_features(img_path)
            if features:
                features_list.append(features)

        # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame
        self.df = pd.DataFrame(features_list)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
        self.df.to_csv("raw_image_features.csv", index=False)

        print(f"\n‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ó–∞–ø–∏—Å–µ–π: {len(self.df)}")
        print(f"   –ö–ª–∞—Å—Å—ã: –õ–∏—Ü–∞ - {self.df['is_face'].sum()}, "
              f"–ù–µ-–ª–∏—Ü–∞ - {len(self.df) - self.df['is_face'].sum()}")

        return True

    def clean_data(self):
        """–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤ –∏ –≤—ã–±—Ä–æ—Å–æ–≤"""
        print("\nüßπ –û–ß–ò–°–¢–ö–ê –î–ê–ù–ù–´–•")
        print("-" * 50)

        if self.df is None:
            print("‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
            return False

        initial_count = len(self.df)

        # 1. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        self.df = self.df.drop_duplicates(subset=['filename'])
        print(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {initial_count - len(self.df)}")

        # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        missing_before = self.df.isnull().sum().sum()
        self.df = self.df.dropna()
        missing_after = self.df.isnull().sum().sum()
        print(f"–£–¥–∞–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏: {missing_before - missing_after}")

        # 3. –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –ø–æ –º–µ–∂–∫–≤–∞—Ä—Ç–∏–ª—å–Ω–æ–º—É —Ä–∞–∑–º–∞—Ö—É
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        numeric_cols = [col for col in numeric_cols if col not in ['is_face']]

        outliers_removed = 0
        for col in numeric_cols:
            Q1 = self.df[col].quantile(0.25)
            Q3 = self.df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            outliers = ((self.df[col] < lower_bound) | (self.df[col] > upper_bound)).sum()
            outliers_removed += outliers

            # –£–¥–∞–ª—è–µ–º –≤—ã–±—Ä–æ—Å—ã
            self.df = self.df[(self.df[col] >= lower_bound) & (self.df[col] <= upper_bound)]

        print(f"–£–¥–∞–ª–µ–Ω–æ –≤—ã–±—Ä–æ—Å–æ–≤ (IQR –º–µ—Ç–æ–¥): {outliers_removed}")

        # 4. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        scaler = StandardScaler()
        for col in numeric_cols:
            if col in self.df.columns:
                self.df[f'{col}_normalized'] = scaler.fit_transform(
                    self.df[[col]]
                )

        print(f"–ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä –Ω–∞–±–æ—Ä–∞: {len(self.df)} –∑–∞–ø–∏—Å–µ–π")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        self.df.to_csv("cleaned_image_features.csv", index=False)

        return True

    def exploratory_analysis(self):
        """–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
        print("\nüîç –ò–°–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–°–ö–ò–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•")
        print("-" * 50)

        os.makedirs("visualizations", exist_ok=True)

        # 1. –ö–û–†–†–ï–õ–Ø–¶–ò–û–ù–ù–ê–Ø –ú–ê–¢–†–ò–¶–ê
        print("1. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã...")
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        correlation_matrix = self.df[numeric_cols].corr()

        plt.figure(figsize=(15, 12))
        sns.heatmap(correlation_matrix,
                    annot=True,
                    cmap='coolwarm',
                    center=0,
                    fmt='.2f',
                    linewidths=1)
        plt.title('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', fontsize=16)
        plt.tight_layout()
        plt.savefig('visualizations/correlation_matrix.png', dpi=150)
        plt.show()

        # –ê–Ω–∞–ª–∏–∑ –≤—ã—Å–æ–∫–∏—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
        high_corr = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i + 1, len(correlation_matrix.columns)):
                if abs(correlation_matrix.iloc[i, j]) > 0.7:
                    high_corr.append((
                        correlation_matrix.columns[i],
                        correlation_matrix.columns[j],
                        correlation_matrix.iloc[i, j]
                    ))

        print(f"   –ù–∞–π–¥–µ–Ω–æ {len(high_corr)} –ø–∞—Ä —Å –≤—ã—Å–æ–∫–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π (>0.7)")
        for corr in high_corr[:5]:  # –ü–æ–∫–∞–∂–µ–º –ø–µ—Ä–≤—ã–µ 5
            print(f"   {corr[0]} ‚Üî {corr[1]}: {corr[2]:.3f}")

        # 2. –î–ò–ê–ì–†–ê–ú–ú–´ –†–ê–°–°–ï–Ø–ù–ò–Ø
        print("\n2. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–∏–∞–≥—Ä–∞–º–º —Ä–∞—Å—Å–µ—è–Ω–∏—è...")

        # –í—ã–±–µ—Ä–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        important_features = ['width', 'height', 'mean_intensity', 'aspect_ratio', 'entropy']

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()

        pairs = [
            ('width', 'height'),
            ('mean_intensity', 'entropy'),
            ('aspect_ratio', 'mean_intensity'),
            ('color_mean_r', 'color_mean_g'),
            ('width', 'aspect_ratio'),
            ('height', 'entropy')
        ]

        for idx, (x_col, y_col) in enumerate(pairs):
            if x_col in self.df.columns and y_col in self.df.columns:
                scatter = axes[idx].scatter(
                    self.df[x_col],
                    self.df[y_col],
                    c=self.df['is_face'],
                    cmap='viridis',
                    alpha=0.6,
                    s=50
                )
                axes[idx].set_xlabel(x_col)
                axes[idx].set_ylabel(y_col)
                axes[idx].set_title(f'{x_col} vs {y_col}')
                axes[idx].grid(True)

        plt.suptitle('–î–∏–∞–≥—Ä–∞–º–º—ã —Ä–∞—Å—Å–µ—è–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π', fontsize=16)
        plt.tight_layout()
        plt.savefig('visualizations/scatter_plots.png', dpi=150)
        plt.show()

        # –í—ã–≤–æ–¥—ã –ø–æ –¥–∏–∞–≥—Ä–∞–º–º–∞–º —Ä–∞—Å—Å–µ—è–Ω–∏—è
        print("   ‚Ä¢ –î–∏–∞–≥—Ä–∞–º–º—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —è–≤–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤")
        print("   ‚Ä¢ –í–∏–¥–Ω–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –º–µ–∂–¥—É —Ä–∞–∑–º–µ—Ä–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–º")
        print("   ‚Ä¢ –¶–≤–µ—Ç–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏")

        return True

    def perform_clustering(self):
        """–ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        print("\nüìä –ü–†–û–í–ï–î–ï–ù–ò–ï –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò")
        print("-" * 50)

        from sklearn.cluster import KMeans
        from sklearn.decomposition import PCA

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        features_for_clustering = [
            'width', 'height', 'mean_intensity',
            'std_intensity', 'entropy', 'aspect_ratio'
        ]

        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        features_for_clustering = [f for f in features_for_clustering if f in self.df.columns]

        X = self.df[features_for_clustering]

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–º–µ—Ç–æ–¥ –ª–æ–∫—Ç—è)
        wcss = []
        k_range = range(2, 11)

        for k in k_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            kmeans.fit(X)
            wcss.append(kmeans.inertia_)

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–∞ –ª–æ–∫—Ç—è
        plt.figure(figsize=(10, 6))
        plt.plot(k_range, wcss, 'bo-')
        plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')
        plt.ylabel('WCSS (Within-Cluster Sum of Square)')
        plt.title('–ú–µ—Ç–æ–¥ –ª–æ–∫—Ç—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')
        plt.grid(True)
        plt.savefig('visualizations/elbow_method.png', dpi=150)
        plt.show()

        # –í—ã–±–∏—Ä–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ k (–Ω–∞–ø—Ä–∏–º–µ—Ä, 3)
        optimal_k = 3
        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(X)

        self.df['cluster'] = clusters

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é PCA
        pca = PCA(n_components=2)
        X_pca = pca.fit_transform(X)

        plt.figure(figsize=(12, 8))
        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1],
                              c=clusters,
                              cmap='tab10',
                              s=100,
                              alpha=0.7,
                              edgecolors='black')

        plt.colorbar(scatter)
        plt.xlabel('PCA Component 1')
        plt.ylabel('PCA Component 2')
        plt.title(f'–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (K={optimal_k}) —Å –ø–æ–º–æ—â—å—é PCA')
        plt.grid(True, alpha=0.3)
        plt.savefig('visualizations/clustering_results.png', dpi=150)
        plt.show()

        # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        print(f"–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ (K={optimal_k}):")
        cluster_stats = self.df.groupby('cluster').agg({
            'is_face': ['mean', 'count'],
            'width': 'mean',
            'height': 'mean',
            'mean_intensity': 'mean'
        })

        print("\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:")
        print(cluster_stats.round(2))

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        print("\nüìå –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –ö–õ–ê–°–¢–ï–†–û–í:")
        for cluster_num in range(optimal_k):
            cluster_data = self.df[self.df['cluster'] == cluster_num]
            face_percentage = cluster_data['is_face'].mean() * 100

            print(f"\n–ö–ª–∞—Å—Ç–µ—Ä {cluster_num}:")
            print(f"  ‚Ä¢ –†–∞–∑–º–µ—Ä: {len(cluster_data)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            print(f"  ‚Ä¢ –õ–∏—Ü–∞: {face_percentage:.1f}%")
            print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {cluster_data['width'].mean():.0f}√ó{cluster_data['height'].mean():.0f}")
            print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —è—Ä–∫–æ—Å—Ç—å: {cluster_data['mean_intensity'].mean():.1f}")

            if face_percentage > 70:
                print(f"  ‚Üí –í–µ—Ä–æ—è—Ç–Ω–æ, —ç—Ç–æ –∫–ª–∞—Å—Ç–µ—Ä —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –ª–∏—Ü")
            elif face_percentage < 30:
                print(f"  ‚Üí –í–µ—Ä–æ—è—Ç–Ω–æ, —ç—Ç–æ –∫–ª–∞—Å—Ç–µ—Ä –±–µ–∑ –ª–∏—Ü")
            else:
                print(f"  ‚Üí –°–º–µ—à–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä")

        return True

    def analyze_distributions(self):
        """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        print("\nüìà –ê–ù–ê–õ–ò–ó –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ô –ü–†–ò–ó–ù–ê–ö–û–í")
        print("-" * 50)

        # –í—ã–±–∏—Ä–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        key_features = ['width', 'height', 'aspect_ratio',
                        'mean_intensity', 'entropy', 'pixel_count']

        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        key_features = [f for f in key_features if f in self.df.columns]

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()

        distributions_info = {}

        for idx, feature in enumerate(key_features):
            if idx < len(axes):
                # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞
                axes[idx].hist(self.df[feature], bins=30, alpha=0.7, color='steelblue', edgecolor='black')
                axes[idx].set_xlabel(feature)
                axes[idx].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
                axes[idx].set_title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {feature}')
                axes[idx].grid(True, alpha=0.3)

                # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
                data = self.df[feature].dropna()
                skewness = data.skew()
                kurtosis = data.kurtosis()

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
                if abs(skewness) < 0.5:
                    dist_type = "–ü—Ä–∏–º–µ—Ä–Ω–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ"
                elif skewness > 0.5:
                    dist_type = "–°–∫–æ—à–µ–Ω–æ –≤–ø—Ä–∞–≤–æ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è –∞—Å–∏–º–º–µ—Ç—Ä–∏—è)"
                else:
                    dist_type = "–°–∫–æ—à–µ–Ω–æ –≤–ª–µ–≤–æ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è –∞—Å–∏–º–º–µ—Ç—Ä–∏—è)"

                distributions_info[feature] = {
                    'skewness': skewness,
                    'kurtosis': kurtosis,
                    'type': dist_type,
                    'mean': data.mean(),
                    'std': data.std()
                }

                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –Ω–∞ –≥—Ä–∞—Ñ–∏–∫
                stats_text = f'Skew: {skewness:.2f}\nKurt: {kurtosis:.2f}'
                axes[idx].text(0.05, 0.95, stats_text,
                               transform=axes[idx].transAxes,
                               verticalalignment='top',
                               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

        plt.suptitle('–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', fontsize=16)
        plt.tight_layout()
        plt.savefig('visualizations/distribution_analysis.png', dpi=150)
        plt.show()

        # –í—ã–≤–æ–¥—ã –ø–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ô:")
        for feature, info in distributions_info.items():
            print(f"\n{feature}:")
            print(f"  ‚Ä¢ –¢–∏–ø —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è: {info['type']}")
            print(f"  ‚Ä¢ –ê—Å–∏–º–º–µ—Ç—Ä–∏—è: {info['skewness']:.3f}")
            print(f"  ‚Ä¢ –≠–∫—Å—Ü–µ—Å—Å: {info['kurtosis']:.3f}")
            print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: {info['mean']:.2f}")
            print(f"  ‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {info['std']:.2f}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è—Ö
        self.report_data['distributions'] = distributions_info

        return distributions_info

    def generate_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ"""
        print("\nüìÑ –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï –û–¢–ß–ï–¢–ê")
        print("-" * 50)

        report_content = f"""
        –û–¢–ß–ï–¢ –ü–û –ú–û–î–£–õ–Æ –ê: –ê–ù–ê–õ–ò–ó –ò –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•
        =================================================

        1. –ò–°–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï
           ‚Ä¢ –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(self.df)}
           ‚Ä¢ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ª–∏—Ü–∞–º–∏: {self.df['is_face'].sum()}
           ‚Ä¢ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –ª–∏—Ü: {len(self.df) - self.df['is_face'].sum()}
           ‚Ä¢ –§–æ—Ä–º–∞—Ç—ã: {self.df['format'].unique()}

        2. –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê
           ‚Ä¢ –£–¥–∞–ª–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã
           ‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∞–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
           ‚Ä¢ –£–¥–∞–ª–µ–Ω—ã –≤—ã–±—Ä–æ—Å—ã –º–µ—Ç–æ–¥–æ–º IQR
           ‚Ä¢ –í—ã–ø–æ–ª–Ω–µ–Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

        3. –ö–û–†–†–ï–õ–Ø–¶–ò–û–ù–ù–´–ô –ê–ù–ê–õ–ò–ó
           ‚Ä¢ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞
           ‚Ä¢ –í—ã—è–≤–ª–µ–Ω—ã —Å–∏–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
           ‚Ä¢ –ù–∞–∏–±–æ–ª–µ–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø–∞—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏

        4. –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø
           ‚Ä¢ –ü—Ä–∏–º–µ–Ω–µ–Ω –º–µ—Ç–æ–¥ K-means
           ‚Ä¢ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
           ‚Ä¢ –í—ã–ø–æ–ª–Ω–µ–Ω–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ PCA
           ‚Ä¢ –î–∞–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞

        5. –ê–ù–ê–õ–ò–ó –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ô
        """

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è—Ö
        if hasattr(self, 'report_data') and 'distributions' in self.report_data:
            for feature, info in self.report_data['distributions'].items():
                report_content += f"""
           ‚Ä¢ {feature}:
             - –¢–∏–ø: {info['type']}
             - –ê—Å–∏–º–º–µ—Ç—Ä–∏—è: {info['skewness']:.3f}
             - –°—Ä–µ–¥–Ω–µ–µ: {info['mean']:.2f} ¬± {info['std']:.2f}
                """

        report_content += f"""

        6. –í–´–í–û–î–´
           ‚Ä¢ –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç —á–µ—Ç–∫–æ —Ä–∞–∑–¥–µ–ª—è–µ–º—ã–µ –∫–ª–∞—Å—Å—ã
           ‚Ä¢ –†–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç —Å –∏—Ö —Ç–∏–ø–æ–º
           ‚Ä¢ –¶–≤–µ—Ç–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏
           ‚Ä¢ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
           ‚Ä¢ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–º–µ—é—Ç —Ä–∞–∑–Ω—É—é –ø—Ä–∏—Ä–æ–¥—É

        7. –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò
           ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
           ‚Ä¢ –£—á–µ—Å—Ç—å –≤—ã—Å–æ–∫–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
           ‚Ä¢ –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        """

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç –≤ —Ñ–∞–π–ª
        with open('module_a_report.md', 'w', encoding='utf-8') as f:
            f.write(report_content)

        print("‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª–µ: module_a_report.md")

        # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–∏–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
        summary = f"""
        –ö–†–ê–¢–ö–ò–ô –û–¢–ß–ï–¢:
        ==============
        –î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}
        –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(self.df)}
        –ò–∑ –Ω–∏—Ö –ª–∏—Ü: {self.df['is_face'].sum()}
        –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: –æ—Ç–ª–∏—á–Ω–æ–µ
        –í—ã–≤–æ–¥—ã: –î–∞–Ω–Ω—ã–µ –ø—Ä–∏–≥–æ–¥–Ω—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        """

        print(summary)

        return True

    def create_requirements_files(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –ö–ó"""
        print("\nüìÅ –°–û–ó–î–ê–ù–ò–ï –§–ê–ô–õ–û–í –ü–û –¢–†–ï–ë–û–í–ê–ù–ò–Ø–ú –ö–ó")
        print("-" * 50)

        required_files = [
            ('module_a_report.md', '–û—Ç—á–µ—Ç –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ'),
            ('raw_image_features.csv', '–ò—Å—Ö–æ–¥–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π'),
            ('cleaned_image_features.csv', '–û—á–∏—â–µ–Ω–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π'),
            ('visualizations/', '–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–º–∏')
        ]

        print("–°–æ–∑–¥–∞–Ω—ã —Ñ–∞–π–ª—ã:")
        for file_path, description in required_files:
            if file_path.endswith('/'):
                os.makedirs(file_path, exist_ok=True)
                print(f"  ‚úì {file_path} - {description}")
            else:
                if not os.path.exists(file_path):
                    with open(file_path, 'w') as f:
                        f.write(f"–§–∞–π–ª {description}")
                    print(f"  ‚úì {file_path} - {description}")
                else:
                    print(f"  ‚úì {file_path} - —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å –∞–Ω–∞–ª–∏–∑–æ–º –¥–∞–Ω–Ω—ã—Ö
        analysis_content = """
        –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• –î–õ–Ø –ú–û–î–£–õ–Ø –ê
        ==========================

        1. –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò –ù–ê–ë–û–†–ê –î–ê–ù–ù–´–•:
           ‚Ä¢ –ò—Å—Ç–æ—á–Ω–∏–∫: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –ø–∞–ø–æ–∫ data/faces –∏ data/non_faces
           ‚Ä¢ –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
           ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: 15+
           ‚Ä¢ –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: is_face (–±–∏–Ω–∞—Ä–Ω–∞—è)

        2. –ö–õ–Æ–ß–ï–í–´–ï –ù–ê–ë–õ–Æ–î–ï–ù–ò–Ø:
           ‚Ä¢ –ß–µ—Ç–∫–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
           ‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏–º–µ—é—Ç —Ä–∞–∑–Ω—É—é –ø—Ä–∏—Ä–æ–¥—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
           ‚Ä¢ –ù–∞–ª–∏—á–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏

        3. –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê:
           ‚Ä¢ –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
           ‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤
           ‚Ä¢ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
           ‚Ä¢ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å

        4. –†–ï–ó–£–õ–¨–¢–ê–¢–´:
           ‚Ä¢ –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
           ‚Ä¢ –í—ã—è–≤–ª–µ–Ω—ã –∑–Ω–∞—á–∏–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
           ‚Ä¢ –ü–æ—Å—Ç—Ä–æ–µ–Ω—ã –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        """

        with open('data_analysis.txt', 'w', encoding='utf-8') as f:
            f.write(analysis_content)

        print(f"  ‚úì data_analysis.txt - –§–∞–π–ª —Å –∞–Ω–∞–ª–∏–∑–æ–º –¥–∞–Ω–Ω—ã—Ö")

        return True

    def run_full_analysis(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        print("\nüöÄ –ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê –î–ê–ù–ù–´–•")
        print("=" * 70)

        steps = [
            ("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö", self.load_and_analyze_data),
            ("–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö", self.clean_data),
            ("–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑", self.exploratory_analysis),
            ("–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è", self.perform_clustering),
            ("–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π", self.analyze_distributions),
            ("–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞", self.generate_report),
            ("–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –ø–æ –ö–ó", self.create_requirements_files)
        ]

        for step_name, step_func in steps:
            print(f"\n‚ñ∂ –®–ê–ì: {step_name}")
            try:
                if not step_func():
                    print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —à–∞–≥–µ: {step_name}")
                    break
            except Exception as e:
                print(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ —à–∞–≥–µ {step_name}: {e}")
                break

        print("\n" + "=" * 70)
        print("‚úÖ –ú–û–î–£–õ–¨ –ê –ó–ê–í–ï–†–®–ï–ù!")
        print("=" * 70)

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–≤–æ–¥–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        if self.df is not None:
            print(f"\nüìä –°–í–û–î–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:")
            print(f"   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(self.df)}")
            print(f"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.df.columns)}")
            print(f"   –ö–ª–∞—Å—Å—ã: –õ–∏—Ü–∞={self.df['is_face'].sum()}, "
                  f"–ù–µ-–ª–∏—Ü–∞={len(self.df) - self.df['is_face'].sum()}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            self.df.to_csv('final_processed_data.csv', index=False)
            print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: final_processed_data.csv")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –º–æ–¥—É–ª—è –ê"""
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
    os.makedirs("visualizations", exist_ok=True)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
    analyzer = ImageDataAnalyzer()
    analyzer.run_full_analysis()

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É –¥–ª—è –æ—Ç—á–µ—Ç–∞
    print("\nüì∏ –°–û–•–†–ê–ù–ï–ù–ò–ï –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ô –î–õ–Ø –û–¢–ß–ï–¢–ê:")

    # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏-–æ—Ç—á–µ—Ç —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    if os.path.exists('visualizations/correlation_matrix.png'):
        img = plt.imread('visualizations/correlation_matrix.png')
        axes[0, 0].imshow(img)
        axes[0, 0].axis('off')
        axes[0, 0].set_title('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞', fontsize=12)

    if os.path.exists('visualizations/scatter_plots.png'):
        img = plt.imread('visualizations/scatter_plots.png')
        axes[0, 1].imshow(img)
        axes[0, 1].axis('off')
        axes[0, 1].set_title('–î–∏–∞–≥—Ä–∞–º–º—ã —Ä–∞—Å—Å–µ—è–Ω–∏—è', fontsize=12)

    if os.path.exists('visualizations/clustering_results.png'):
        img = plt.imread('visualizations/clustering_results.png')
        axes[1, 0].imshow(img)
        axes[1, 0].axis('off')
        axes[1, 0].set_title('–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏', fontsize=12)

    if os.path.exists('visualizations/distribution_analysis.png'):
        img = plt.imread('visualizations/distribution_analysis.png')
        axes[1, 1].imshow(img)
        axes[1, 1].axis('off')
        axes[1, 1].set_title('–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π', fontsize=12)

    plt.suptitle('–í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò –î–õ–Ø –û–¢–ß–ï–¢–ê –ü–û –ú–û–î–£–õ–Æ –ê', fontsize=16)
    plt.tight_layout()
    plt.savefig('report_visualizations_summary.png', dpi=150, bbox_inches='tight')
    plt.show()

    print(" –í—Å–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ 'visualizations/'")
    print(" –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏: 'report_visualizations_summary.png'")


if __name__ == "__main__":
    main()